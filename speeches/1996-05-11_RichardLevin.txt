It is a great honor to have been asked to return to Oxford to give the first annual Dalton Lecture at Manchester College and to celebrate your new designation as a Full Chartered College of Oxford University. It is an especially great honor to present these remarks before such a distinguished audience of educational and industrial leaders.

Manchester shares with Yale University an 18th century foundation arising from a desire for religious freedom. Manchester’s Nonconformist founders sought to provide higher education to those excluded from Oxford and Cambridge by the required “confessions of faith,” and the new college was open to young men of every religious denomination. Yale’s founders were even more removed from the Church of England, but they were decidedly less liberal than your own. They feared that the theological training available at Harvard lacked the rigor of its Puritan fathers and threatened the moral fabric of New England. Yale’s early mission was to provide suitably instructed leadership for the congregational churches and the Civil State.

This lecture honors the memory of Manchester’s great chemist, John Dalton, a Quaker from Cumbria who spent his most productive years at this college in Manchester from 1793 to 1803, when the dissenting academy moved for a time to York. Dalton himself remained in Manchester long enough to see the College return in 1840. He died in 1844.

An indifferent reception to Dalton’s first and least notable book, Meteorological Observations and Essays, did not deter him from recording an average of ten observations of the weather each day throughout the course of his life. It is not for this compulsive attention to detail, however, that he is best known. Rather, his distinction rests on developing and giving plausibility to the ancient theory that matter is composed of tiny building blocks called atoms that differ in size and weight across the various elementary types of matter. His theories and evidence, regarded from a modern perspective, are almost entirely wrong in the details. But his central idea was emphatically correct, and it provided the foundation for the development of chemistry in the nineteenth century.

As the first Dalton Lecturer, I would hope to avoid emulating the master by getting the small details wrong, but I shall be delighted if I am able to get the central ideas right!

* * * * *

I have chosen as my topic for today “The Future of the University.” Let me circumscribe it a bit. In my ignorance, I shall confine myself to the American university. Even then I shall focus principally upon two aspects of its future: (1) the strength and importance of our national commitment to basic scientific research undertaken in universities and (2) the viability of our continued commitment, as we enter the twenty-first century, to a nineteenth century conception of liberal education.

I speak at a time when the value of the American universities’ contribution to the broader society is questioned by many of its citizens. Some worry - anxiously and very publicly - that the nation’s finest colleges undermine, rather than preserve, its cultural heritage. And even those who understand and admire the accomplishments of universities are concerned that higher education may be a luxury that the society cannot afford.

What leads me to say that the American public questions the mission and performance of its universities? As an economist, it is my habit to look first at the numbers. Presumably, in a democracy, we can learn something about the public’s will from the pattern of public expenditures.

Consider the government’s support for students. You should be mindful that, in the United States, most students pay their own way through college. A minority are given grants, which come in part from the government, in part from the endowments and operating resources of the universities themselves. Thus, the government provides students with grants, based principally on financial need, that cover only a small fraction of the total cost of higher education. In mid-1970s, the average Federal grant awarded to qualified undergraduates around the country reached its highest point in inflation-adjusted dollars; it has declined more or less steadily since then. In 1978, the Federal government contributed 25 cents of every dollar of grant aid awarded to Yale College students. Today, the government contributes only 7 cents of every dollar. At the very small number of schools that share Yale’s commitment to fund the full financial need of all admitted students, the shortfall in grant aid is covered by the institution’s own resources. For most college students around the country, however, the reduced availability of Federal grants has led to increased dependence on student loans. To make matters worse, the Republican leadership of the Congress has threatened to eliminate the long-standing policy of forgiving interest on Federal loans during the period a student is still enrolled in school. This in-school interest subsidy was retained in the compromise recently reached on the fiscal 1996 Federal budget, but its elimination, which may yet occur, would raise the ultimate debt burden facing many of our college graduates, which is already very high, by 25 to 60%.

Consider also public support for university-based scientific research. Although aggregate funding has grown at about 5% per year since 1978, a closer look reveals two indications of diminished confidence in our universities. First, there has been some movement away from support for the type of basic or fundamental scientific investigation in which universities have unique competence. This has been accompanied by increased emphasis on research that offers greater possibilities for immediate practical or commercial application.

Second, both Congress and the Executive Branch have begun to challenge the principle that has governed national science policy since the late 1940s - that universities, subject to careful auditing and a competitive system of peer review, should recover the full cost of Federally-sponsored scientific research. Increasingly, academic institutions have been forced to rely on their own resources to support scientific research, and an increasing share of research dollars have been allocated through the exchange of political favors instead of competition on the merits.

Where else do we find evidence of public skepticism about the mission and performance of America’s universities? In newspapers and news magazines, on talk shows, in letters from alumni, in the halls of Congress, even in some prestigious journals of opinion, we see a number of complaints repeated frequently: the cost of a college education is too high; professors spend too little time teaching; universities should be doing more for the communities that surround them. There are important things to say about each of these issues, but I will focus on two particular complaints about the nature and quality of undergraduate education in America.

One widely-held concern is that after four years of college, graduates are not prepared to do anything useful. Although this claim would seem to be refuted by the abundant evidence that the rate of return to a college education increased dramatically during the 1980s, consider this statement from John McArthur, the recently retired Dean of the Harvard Business School: “In Northern Europe or Japan, they have technical high schools that focus on real careers in real industries, giving world-class technical education followed by apprenticeship programs. Students end up in their early 20s able to compete with all comers in the world. Here people go on to something called college, and chances are good after they finish that they are no closer to a [good career] than they were before.”1

A second concern, perhaps less widely held but even more fervently expressed, is that students are encouraged, even bullied, by their teachers to reject the cultural heritage of Western Civilization, to shun the great books of the West in the name of multiculturalism, and to accept “politically correct” orthodoxies. This concern is voiced frequently on the editorial pages of the Wall Street Journal, and in letters to university presidents. The dangers of politicizing the curriculum and relaxing standards of critical judgment have also been described by many thoughtful commentators within the academy - among them David Bromwich, Housum Professor of English at Yale, whose brilliant book of essays, Politics by Other Means, I would recommend to anyone interested in understanding America’s “culture wars.”

In this atmosphere of skepticism and challenge, two important features of the late twentieth century American university are seriously called into question: (1) our national commitment to excellence in basic scientific research and scientific education, and (2) our commitment to the broad, liberal education of our undergraduates. I wish, in the balance of this lecture, to defend these commitments and to advance the claim that they are essential to the future health of, in turn, the American economy and the American polity.

* * * * *

At the end of the Second World War, the United States government established, clearly and self-consciously, an unprecedented and heavily-subsidized system for the support of scientific research and education. The essential elements of the system were described by Vannevar Bush, Science Advisor to President Truman, in a deservedly famous 1946 report entitled Science: The Endless Frontier.

As the system developed, the Federal Government assumed the principal responsibility for the financial support of basic scientific research - investigations motivated by curiosity and the quest for knowledge, without a clear practical or commercial objective. There were, of course, some exceptions. Some very large corporations - especially those with a significant degree of market power such as AT&T, DuPont, and later IBM - financed substantial basic research operations, but throughout the post-war period the Federal government has always paid for at least three-quarters of the nation’s basic research effort.

Applied research and development, directed toward the commercial application of new scientific knowledge, was financed by the private sector, with a few notable exceptions. One exception, involving relative modest amounts of federal investment, was research directed toward agricultural improvements, a function the Federal government had assumed as far back as 1875, when the first agricultural experimentation station was established in Yale’s home city of New Haven, Connecticut. The other principal exception, far more important quantitatively, was military research and development, where the commercial application was directed to the needs of a single customer - the government itself.

Another major strategic decision made after World War II concerned not the source of financing for research and development, but the institutions in which it was carried out. Applied research and development - whether financed privately or by the Department of Defense - was largely performed by private firms, although there remained a few important government laboratories devoted to applied research on weapons. But the preponderance of basic research, which was chiefly supported by the Federal government, was conducted in universities.

The decentralized performance of basic research in universities has had several important features. First, although the Federal budgetary process determines the amount available to support research in the various fields of basic science, decisions on the allocation of funds within fields are left, in most instances, to the judgment of committees of independent experts, or scientific peers. Second, to provide universities with adequate incentive to invest in facilities, infrastructure, and support staff, the government has allowed the universities to recover not simply the direct cost of research, as is customary in the U.K., but the fully allocated cost of research. This fully allocated cost includes appropriate portions of the cost of operating and replacing laboratory buildings, the cost of maintaining relevant library collections and communications infrastructure, and the cost of necessary support staff. Finally, by locating fundamental scientific research in universities rather than in government laboratories or specialized research institutes, the next generation of scientists receives its education and training from the nation’s best scientists, who are required to teach as they pursue their own research.

This system of organizing science, which makes university the locus of fundamental research, has, on its own terms and in international comparative perspective, been an extraordinary success. Without meaning to sound chauvinistic, it is fair to say that the United States is the world’s leader in basic research and scientific education. Over the past two decades, the United States has been the source of 35% of all scientific publications worldwide. Since 1975 more than 60% of the world’s Nobel prizes have been awarded to Americans or to foreign nationals working in American universities. And approximately half the students worldwide who leave their home countries for an advance degree in science or engineering come to the U.S. universities.

America’s competitive advantage in science, arising from its unique system or organization and historically generous public funding, has had an enormous impact on its position in the world economy. It would require another lecture to document this claim adequately, and it is a lecture I have given before. Its essence is that the United States typically dominates industries that are based on newly-created knowledge. For instance, America had a dominant share of the world market in the early years of the development of jet aircraft, televisions, computers, semiconductors, and synthetic fibers. As technologies mature, other nations catch up to America technologically and sometimes lower production costs below prevailing U.S. levels. Today, however, America continues to lead the world in industries based on new knowledge - notably in software, advanced communications equipment, medical equipment, and pharmaceuticals developed using the research methods of modern genetics and molecular biology.

Against this spectacular record of achievement, the pervasive skepticism in Washington is difficult to comprehend. It is in part attributable to the pressing need to balance the Federal budget, in part attributable to the much exaggerated abuse of government funds at one major research university a few years ago, in part attributable to the general climate of distrust in which universities now find themselves. The 1994 Republican party “Contract with America” called for real reductions of 33% in Federal funds for scientific research by 2002. The actual cuts in spending to date have been much less severe, especially in the biomedical area. Nonetheless, if overall national spending on basic research fails to keep pace with inflation, there will be good reason to worry about the maintenance of America’s scientific leadership, and, ultimately, its economic strength.

One particularly troublesome development is the tendency to substitute Congressional judgment for peer review in making awards for large research project grants. The percentage of basic research funds awarded directly by legislative “ear-marking” rose from 0.3% in 1980 to 6.4% in 1992. This practice has been curbed somewhat over the past four years, but it should be eliminated entirely. Supporting research on the basis of political influence rather than competition on the merits is a certain recipe for mediocre science.

There is also a disturbing erosion of the principle that allows universities to recover the full cost of supporting research. In recent years, the government has ceased to reimburse universities for certain categories of costs incurred in undertaking research. It has also imposed an arbitrary cap on reimbursements for the cost of administrative support of research. Despite evidence that many capital and operating costs of facilities supporting research in public universities are borne directly by state governments and thus not claimed for reimbursement on Federal grants, some Congressmen continue to rail against “unexplained” discrepancies in the costs recovered by institutions. Despite repeated attempts by the universities to explain their position, and despite recent studies that show facility and administrative costs in industrial research operations to be as high or higher than those in universities, further Congressional action to limit the recovery of research costs is an ever-present danger.

Even more serious is the tendency, encouraged by the Reagan-Bush and Clinton administrations alike, to favor government sponsorship of applied, commercially-oriented research at the expense of basic science. Much of this applied research is allegedly “generic” or “pre-competitive,” and much of it involves the faddish collaboration of universities and industries under government sponsorship. As one who spent more than a decade studying the economics of industrial research and development prior to becoming a university administrator, let me assure you that the movement away from government support for basic research toward government-sponsored applied research and university-industrial partnerships is a movement in precisely the wrong direction.

Here are four reasons why. First, the prior performance of the U.S. government in supporting commercially-oriented applied research is overwhelmingly dismal. Dozens of economic studies, whether commissioned by the government or undertaken privately, support this conclusion. Virtually the only successful applied R & D efforts of the U.S. government have been in instances - such as weapons systems and early computing systems - where the government is the sole customer for the resulting products. The market performs far better than the government in selecting applied research projects with high payoffs. I make this point not to discredit the very valuable and important research partnerships that have been created between companies and universities. There has been great benefit in this area. My reservations are limited to government intervention to manage or direct such university-industry partnerships.

Second, the returns from investment in basic research are extraordinarily high. Though it is not easy to estimate the social rate of return to investment in basic research, almost every study published finds returns in the 25 to 50% range, well in excess of the average rate of return on applied research and development.

Third, the private sector tends to under-invest in basic research because the returns from the creation of new generic knowledge are difficult to appropriate for private benefit. On the other hand, it is much easier to reap the returns from investment in applied research directed toward a specific commercial end, especially if the legal framework governing intellectual property provides effective protection against the imitation of one’s products by rivals.

Finally, basic research, which is undertaken primarily in our universities, is the source from which all applied research and development ultimately flows. The time lags are long, far longer than an impatient private sector could tolerate. Ordinarily, the ultimate commercial applications are entirely unforeseen when the initial, enabling discoveries are made in university laboratories. But think of the consequences. It is more than thirty years since Watson and Crick discovered the double helix, and the enormous practical benefits of this discovery are only now beginning to be realized through new medical treatments and a whole new technology for developing pharmaceuticals. If we fail to maintain investment in basic science, the cost will not be measured in the economic performance of the next five or ten years. It will be measured in the economic performance of the next fifty years and the next century.

Let me close this section of my remarks with one illustration of how investment in basic research yields enormous and entirely unanticipated benefits. Let me tell you the story of Professor William Bennett, who began working in the 1950s on the phenomenon of coherent light. After he came to Yale in 1961, he continued his work on lasers with the support of grants from the U.S. Department of Defense. Professor Bennett says the Russians found it astonishing that the American government funded basic research of this type, research that appeared to lead nowhere in terms of practical or useful application. For many years, the laser was what Professor Bennett calls “a solution looking for a problem.”

Today there are so many uses for lasers that it would be impossible to describe them all in the time that remains. Lasers are used to cut cloth, to lay out the foundations of a house, to make micro chips, to pinpoint and treat brain tumors without surgery or irradiation of the whole head. We run lasers into arteries to clear them of plaque.

When I spoke recently to Professor Bennett, he was at home recovering from treatment for a detached retina. He told me that the treatment was accomplished by using precisely the same Argon Ion Laser which he developed at Yale in 1964!

* * * * *

Let me now return to the other threat to the American university that I earlier identified - the attack on liberal education - and begin by placing it in proper historical context. I noted two themes emphasized by the critics of liberal education - skepticism about the practical utility of a college education and fear that universities might undermine prevailing values. It is not difficult to demonstrate that these are recurrent themes in American history.

Alexis de Tocqueville, the young Frenchman who traveled to America in 1831, observed in his brilliant commentary, Democracy in America , the distinctively American skepticism about the value of knowledge for its own sake. He noted that Americans pursued scientific knowledge eagerly, but not to edify the intellect. Rather, they sought knowledge as a means to material and physical comfort. As he put it, rather bluntly: “In aristocratic ages science is more particularly called upon to furnish gratification to the mind; in democracies, to the body.”2

Ambivalence about intellectual pursuits divorced from practical, material ends and concern about the morally corrosive effects of such pursuits have been persistent features of American culture. Motivated by the hostility shown to intellectuals during the McCarthy investigations of the 1950s, Richard Hofstadter published his Pulitzer Prize-winning history, Anti-Intellectualism in American Life, in which he traced the roots of our national skepticism about learning to the evangelicalism of the Great Awakening, through subsequent religious revivals, Jacksonian democracy, and recurrent outbursts of populism. He concluded that “anti-intellectualism is, in fact, older than our national identity. … [R]egard for intellectuals in the United States has not moved steadily downward … , but is subject to cyclical fluctuations.”3

The uniquely mixed American system of higher education reflects the national ambivalence about the life of the mind. The land grant colleges, which have grown to become our large, publicly-owned and controlled universities, were founded with the presumption that they would contribute directly to the useful arts, especially agriculture, through practical, results-oriented experimentation and dissemination, and their instructional methods were directed toward the education of pragmatic citizen-farmers. The older private universities and liberal arts colleges, though not unaffected by the American proclivity for pragmatism, were much closer in purpose and method to their European ancestors.4 Indeed, there is a direct line of influence running from the medieval University of Paris to Oxford to Cambridge to Harvard to Yale, and from Yale to Princeton, Columbia, Williams, Cornell, Johns Hopkins, and the University of Chicago, among others.

Against this historical background, let me respond to these characteristically American fears about the university by asking first: what is the case for education divorced from immediate practical ends? Or, in other words, how can a liberal education be justified today? Then I shall proceed to discuss the second issue: what should be the content of the curriculum?

Although Cardinal Newman had distinctly different ideas about truth and moral development than many who work in universities today, his definition of a liberal education remains highly relevant. As he stated in The Idea of a University, education is “liberal” when it is an end in itself, independent of practical consequences, directed to no specific purpose other than the free exercise of the mind.5 Liberal education cultivates the intellect and expands the capacity to reason and to empathize. Its object is not to convey any particular content, but to develop certain qualities of mind - the ability to think critically and independently, to liberate oneself from prejudice, superstition, and dogma.

Although the purpose of a liberal education is to develop habits of mind and not to acquire specific or “useful” knowledge, even Cardinal Newman recognized that liberal education could be defended on utilitarian grounds because it produces citizens who can make a genuine contribution to society.

“Training of the intellect,” Newman observes, “which is best for the individual himself, best enables him to discharge his duties to society. … If … a practical end must be assigned to a University course, I say it is that of training good members of society.” Newman continues: “It is the education which gives a man a clear conscious view of his own opinions and judgments, a truth in developing them, an eloquence in expressing them, and a force in urging them. It teaches him to see things as they are, to go right to the point, to disentangle a skein of thought, to detect what is sophistical, and to discard what is irrelevant. It prepares him to fill any post with credit, and to master any subject with facility.”6

Is this case as compelling today as it was one hundred and fifty years ago when Newman advanced it? I think it is. Of course, one might argue that in the Age of Information a properly educated person needs to master a specific and substantial body of information. But I would disagree. Indeed, the hallmark of the Age of Information is the astonishing ease with which one can acquire the information one needs when one needs it. The capacity to make fruitful use of vast quantities of information is what we really want our students to acquire. And that is precisely the object of a liberal education, to develop the capacity to reason independently, to sift through information and extract what is useful and, to use Newman’s words, “to discard what is irrelevant.” Presumably, one does not wish to produce for the twenty-first century businessmen and women who know nothing more than the technical tools of accounting and finance, nor politicians who know nothing more than the techniques of effective communication. One hopes that the leaders of the next century will have the ability to think independently and creatively, and, moreover, one hopes that they will have formed that ability in the course of reflecting on questions broader than those involved in mastering the technologies of their respective callings.

This brings us to the next question: what should be the content of a curriculum designed to provide a liberal education? There is a widely publicized view that American higher education has rejected the wisdom found in the great works of Western Civilization as irrelevant, culturally biased, and “politically incorrect.” This view not only fails to grasp what is actually going on in our universities; it also fails to grasp what is going on in the great works of Western Civilization.

In fact, the great works of Western philosophy and political theory provide the intellectual foundation, and the subtlest critiques, of the institutions that dominate the economy and society in which we live: representative government, the rule of law, individual rights, free markets. This alone suffices to make the Western tradition an essential subject of study. But it is also true that we, and I mean “we” to be inclusive of all literate human beings, understand and realize ourselves more completely as persons if we read and confront the great works of Western literature. None of us is Oedipus or Hamlet or Emma Woodhouse or Anna Karenina, but their existence enriches ours, posing for us in human terms the most profound questions of morality and aesthetics, providing representations in terms of which we define ourselves.

Now it is true that even at Yale some scholars and teachers choose to emphasize the darker side of Western history - the institution of slavery, the subordination of women, the conquest and destruction of native cultures, and environmental degradation. These are, after all, features of our history, and they need to be confronted and understood. The subjects of race, gender, and colonialism elicit strong, sometimes irrational reactions from those who sympathize with the historically disadvantaged, and they surely are worthy of study. But this is not to say that the lenses of race, gender, and colonialism are the only ones through which Western experience is or should be seen nowadays, and they are certainly not the exclusive, nor the prevailing, perspective from which the West is regarded in the bulk of Yale’s courses. Instead, old-fashioned as it may seem, most of our undergraduate courses on Shakespeare or Wordsworth, Plato or Kant, still wrestle with the question: what is the author trying to tell us about human experience?

We should of course be deeply concerned about excessive politicizing of the curriculum, about choosing literary, historical, and philosophical texts for the “correctness” of their point of view rather than their inherent quality. Yet many of those who fear that our students are being indoctrinated by the so-called “politically correct” have a terribly naive view of what it means to study Western Civilization. They believe that studying Western culture is akin to taking an oral vaccine: by ingesting the great works of the Western tradition and “appreciating” them, one absorbs the values located therein and resists infection by foreign ideas.

We should remember that the great works of the West, taken as a whole, do not present a unified system of values. They converse with one another. Aristotle responds to Plato as Virgil responds to Homer as Milton responds to Virgil. Indeed, as my Yale colleague Harold Bloom argues, all strong poets define themselves against a predecessor, a “poetic father.” But a common feature of the canonical works is what Bloom calls their strangeness, their profound originality. Each focuses on the human condition through a different lens.

Nor are these works, taken individually, didactic. They don’t contain a doctrine, a codification of values, a set of precepts about how one should live. We don’t look to them for practical advice. We don’t come away from a reading of Oedipus Rex determined to ask for identification the next time we find it necessary to slay a stranger we encounter on the road. As Socrates observed, virtue is not a craft. One cannot learn how to live the good life from an instruction manual; it requires active engagement, thinking for oneself about one’s situation.

Why then are the Great Books great? They are great precisely because they challenge us to think for ourselves. They wrestle with the deepest and most difficult questions concerning human experience and moral behavior, and they are so rich in their characterization of that experience and behavior that they are open to profound differences in interpretation. They challenge us, each individual and each generation, to reinterpret them so that they become part of our own view of humanity and the world. And this, of course, is exactly why the great works of the West deserve a central role in the curriculum of a liberal education. Unlike works that are less enduring or more limited in scope, and because they are so challenging and problematic, the great works of the Western tradition are ideal materials for developing the reader’s capacity to think rigorously and independently. And the development of this capacity is, as a long line of educators descended from Cardinal Newman has claimed, the principal object of liberal education.

At the core of America’s ambivalence about the mission of its universities is an apparent contradiction in expectations. Americans expect universities to preserve the cultural heritage, to pass it on to the next generation, and to educate the young to assume responsible positions of leadership. Thus, the public expects universities to perform the work of socialization. But it also expects universities to be oases of free inquiry and free expression, safe harbors wherein the young can test their ideas, experiment, and explore. How can Americans be confident that, given the luxury of freedom to question everything, the young will resist the temptation to overthrow the established order and emerge as responsible citizens?

Thomas Jefferson’s answer was simple: in a regime of unfettered inquiry guided by reason, truth emerges. Thus, we should accept the conclusions of free and autonomous individuals of good will. Of course, we run the risk that received values and beliefs will be rejected. But educators can minimize the risk of unwise decisions by strengthening each individual’s capacity to reason and by protecting free inquiry and open debate.

Perhaps Jefferson’s answer seems a little too simple today. Perhaps we are a little more skeptical than our Enlightenment forbears. But is there a serious alternative? Ultimately, the reason we believe in the mission of our universities is no different from the reason we believe in democracy. The fear that students will abuse their freedom by rejecting what is best in the culture they inherit is really no different from the fear that voters will abuse their freedom by electing the incompetent, the demagogue, or the tyrant. Can we really trust the people’s judgment? Here, too, the answer is that having faith in the judgment of free, autonomous individuals is better than the alternative, which is certain tyranny.

* * * * *

I close by asking: what should the American university seek to accomplish in the twenty-first century? With an appropriate and deserved commitment from the government to persist in extending the frontiers of scientific knowledge, the University can contribute mightily to domestic and global prosperity and health. And, as for our educational objectives, are not the words of Jefferson still apt? Should we not seek “to develop the reasoning faculties of our youth, [to] enlarge their minds, … And, generally, to form them to habits of reflection and correct action, rendering them examples of virtue to others, and of happiness within themselves.”